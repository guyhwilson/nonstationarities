{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import glob, sys\n",
    "\n",
    "\n",
    "[sys.path.append(f) for f in glob.glob('../utils/*')]\n",
    "from preprocess import DataStruct\n",
    "from plotting_utils import figSize\n",
    "from lineplots import plotsd\n",
    "from hmm_utils import *\n",
    "from hmm import *\n",
    "from session_utils import *\n",
    "from recalibration_utils import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "\n",
    "min_nblocks = 3       # min number of blocks for a session to be include\n",
    "max_ndays   = 30      # accept all pairs of sessions regardless of time between\n",
    "#min_R2      = 0.1      # subselect days with good decoder transfer performance \n",
    "\n",
    "data_dir    = '/oak/stanford/groups/shenoy/gwilson/nonstationarities/'\n",
    "participant = 'T5/historical/'\n",
    "save_path   = '/oak/stanford/groups/shenoy/gwilson/nonstationarities/T5/train/'\n",
    "files       = loadDataset(data_dir, participant)\n",
    "\n",
    "\n",
    "# now preprocess by getting \"good\" sessions and removing those with low block counts:\n",
    "sessions_check = np.load('../utils/misc_data/NewSessions_check.npy', allow_pickle = True).item()\n",
    "files          = get_Sessions(files, min_nblocks, manually_remove = sessions_check['bad_days'])\n",
    "\n",
    "init_pairs    = get_SessionPairs(files, max_ndays = max_ndays)\n",
    "pairs         = init_pairs\n",
    "n_pairs       = len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/software/user/open/py-scikit-learn/0.19.1_py36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 % complete\n",
      "1.5 % complete\n",
      "2.2 % complete\n",
      "2.9 % complete\n",
      "3.7 % complete\n",
      "4.4 % complete\n",
      "5.1 % complete\n",
      "5.9 % complete\n",
      "6.6 % complete\n",
      "7.4 % complete\n",
      "8.1 % complete\n",
      "8.8 % complete\n",
      "9.6 % complete\n",
      "11.0 % complete\n",
      "11.8 % complete\n",
      "12.5 % complete\n",
      "13.2 % complete\n",
      "14.0 % complete\n",
      "14.7 % complete\n",
      "15.4 % complete\n",
      "16.2 % complete\n",
      "16.9 % complete\n",
      "17.6 % complete\n",
      "18.4 % complete\n",
      "19.1 % complete\n",
      "19.9 % complete\n",
      "21.3 % complete\n",
      "22.1 % complete\n",
      "22.8 % complete\n",
      "23.5 % complete\n",
      "24.3 % complete\n",
      "25.0 % complete\n",
      "25.7 % complete\n",
      "26.5 % complete\n",
      "27.2 % complete\n",
      "27.9 % complete\n",
      "28.7 % complete\n",
      "29.4 % complete\n",
      "30.1 % complete\n",
      "31.6 % complete\n",
      "32.4 % complete\n",
      "33.1 % complete\n",
      "33.8 % complete\n",
      "34.6 % complete\n",
      "35.3 % complete\n",
      "36.0 % complete\n",
      "36.8 % complete\n",
      "37.5 % complete\n",
      "38.2 % complete\n",
      "39.0 % complete\n",
      "39.7 % complete\n",
      "40.4 % complete\n",
      "41.9 % complete\n",
      "42.6 % complete\n",
      "43.4 % complete\n",
      "44.1 % complete\n",
      "44.9 % complete\n",
      "45.6 % complete\n",
      "46.3 % complete\n",
      "47.1 % complete\n",
      "47.8 % complete\n",
      "48.5 % complete\n",
      "49.3 % complete\n",
      "50.0 % complete\n",
      "50.7 % complete\n",
      "52.2 % complete\n",
      "52.9 % complete\n",
      "53.7 % complete\n",
      "54.4 % complete\n",
      "55.1 % complete\n",
      "55.9 % complete\n",
      "56.6 % complete\n",
      "57.4 % complete\n",
      "58.1 % complete\n",
      "58.8 % complete\n",
      "59.6 % complete\n",
      "60.3 % complete\n",
      "61.0 % complete\n",
      "62.5 % complete\n",
      "63.2 % complete\n",
      "64.0 % complete\n",
      "64.7 % complete\n",
      "65.4 % complete\n",
      "66.2 % complete\n",
      "66.9 % complete\n",
      "67.6 % complete\n",
      "68.4 % complete\n",
      "69.1 % complete\n",
      "69.9 % complete\n",
      "70.6 % complete\n",
      "71.3 % complete\n",
      "72.8 % complete\n",
      "73.5 % complete\n",
      "74.3 % complete\n",
      "75.0 % complete\n",
      "75.7 % complete\n",
      "76.5 % complete\n",
      "77.2 % complete\n",
      "77.9 % complete\n",
      "78.7 % complete\n",
      "79.4 % complete\n",
      "80.1 % complete\n",
      "80.9 % complete\n",
      "81.6 % complete\n",
      "83.1 % complete\n",
      "83.8 % complete\n",
      "84.6 % complete\n",
      "85.3 % complete\n",
      "86.0 % complete\n",
      "86.8 % complete\n",
      "87.5 % complete\n",
      "88.2 % complete\n",
      "89.0 % complete\n",
      "89.7 % complete\n",
      "90.4 % complete\n",
      "91.2 % complete\n",
      "91.9 % complete\n",
      "93.4 % complete\n",
      "94.1 % complete\n",
      "94.9 % complete\n",
      "95.6 % complete\n",
      "96.3 % complete\n",
      "97.1 % complete\n",
      "97.8 % complete\n",
      "98.5 % complete\n",
      "99.3 % complete\n",
      "100.0 % complete\n"
     ]
    }
   ],
   "source": [
    "from hmm import *\n",
    "from hmm_utils import prep_HMMData, get_DiscreteTargetGrid, train_HMMRecalibrate\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "\n",
    "\n",
    "# general settings:\n",
    "diffs           = list()\n",
    "task            = None\n",
    "train_size      = 0.67\n",
    "sigma           = 2\n",
    "gridSize        = 20\n",
    "\n",
    "for i, (A_file, B_file) in enumerate(pairs):\n",
    "    dayA = DataStruct(A_file, alignScreens = True, causal_filter = 2)\n",
    "    dayB = DataStruct(B_file, alignScreens = True, causal_filter = 2)\n",
    "    \n",
    "\n",
    "    #dayA_blocks             = [sessions_check[A_file] if A_file in sessions_check.keys() else None][0]\n",
    "    #dayB_blocks             = [sessions_check[B_file] if B_file in sessions_check.keys() else None][0] \n",
    "    #dayA_task, dayB_task, _ = getPairTasks(dayA, dayB, task = task)\n",
    "    dayA_blocks, dayB_blocks = None, None\n",
    "    dayA_task, dayB_task    = None, None\n",
    "\n",
    "    # obtain features and cursorError targets:\n",
    "    \n",
    "    Atrain_x, Atest_x, Atrain_y, Atest_y  = getTrainTest(dayA, train_size = train_size, sigma = None, blocks = dayA_blocks, task = dayA_task, returnFlattened = True)    \n",
    "    Atrain_x, Atest_x  = get_BlockwiseMeanSubtracted(Atrain_x, Atest_x, concatenate = True)\n",
    "    Atrain_y           = np.concatenate(Atrain_y)\n",
    "    Atest_y            = np.concatenate(Atest_y)\n",
    "\n",
    "    Btrain_x, Btest_x, Btrain_y, Btest_y, B_cursorPos, _  = getTrainTest(dayB, train_size = train_size, sigma = sigma, blocks = dayB_blocks, task = dayB_task, \n",
    "                                                                         returnFlattened = True, returnCursor = True)    \n",
    "\n",
    "    Btrain_x, Btest_x  = get_BlockwiseMeanSubtracted(Btrain_x, Btest_x, concatenate = True)\n",
    "    Btrain_y           = np.concatenate(Btrain_y)\n",
    "    Btest_y            = np.concatenate(Btest_y)\n",
    "    B_cursorPos        = np.concatenate(B_cursorPos)\n",
    "    targetPos          = Btrain_y + B_cursorPos\n",
    "\n",
    "    A_decoder_score, A_decoder = traintest_DecoderSupervised([Atrain_x], [Atrain_x], [Atrain_y], [Atrain_y], meanRecal = False)    \n",
    "\n",
    "    # add smoothing and session-specific information\n",
    "    pair_data = dict()\n",
    "    pair_data['A_file']     = A_file\n",
    "    pair_data['B_file']     = B_file\n",
    "    pair_data['days_apart'] = daysBetween(dayA.date, dayB.date)\n",
    "    pair_data['task']       = None\n",
    "    pair_data['train_size'] = 0.67\n",
    "    pair_data['smoothing']  = 2\n",
    "    \n",
    "    pair_data['A_decoder_score'] = A_decoder_score\n",
    "    pair_data['A_decoder']       = A_decoder\n",
    "    \n",
    "    pair_data['A_train_neural']  = Atrain_x\n",
    "    pair_data['A_test_neural']   = Atest_x\n",
    "    pair_data['A_train_targvec'] = Atrain_y\n",
    "    pair_data['A_test_targvec']  = Atest_y\n",
    "    \n",
    "    pair_data['B_train_neural']  = Btrain_x\n",
    "    pair_data['B_test_neural']   = Btest_x\n",
    "    pair_data['B_train_targvec'] = Btrain_y\n",
    "    pair_data['B_test_targvec']  = Btest_y\n",
    "    pair_data['B_train_cursor']  = B_cursorPos\n",
    "    pair_data['B_targLocs']      = get_DiscreteTargetGrid(dayB, gridSize = gridSize, task = dayB_task)\n",
    "    \n",
    "    \n",
    "    save_fname = dayA.date + '_to_' + dayB.date + '.npy'\n",
    "    np.save(save_path + save_fname, pair_data)\n",
    "    \n",
    "    if (i + 1) % int(np.round(len(pairs) / 10)):\n",
    "        print(np.round((i + 1) * 100 / len(pairs), 1), '% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
