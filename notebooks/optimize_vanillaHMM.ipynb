{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below does a grid search for optimal HMM values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "[sys.path.append(f) for f in glob.glob('../utils/*')]\n",
    "from preprocess import DataStruct\n",
    "from plotting_utils import figSize\n",
    "from lineplots import plotsd\n",
    "from session_utils import *\n",
    "from recalibration_utils import *\n",
    "from click_utils import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "\n",
    "\n",
    "min_nblocks    = 3       # min number of blocks for a session to be include\n",
    "max_ndays      = 30      # accept all pairs of sessions regardless of time between\n",
    "#min_R2         = 0.1    # subselect days with good decoder transfer performance \n",
    "\n",
    "\n",
    "f_dir          = glob.glob('D:/T5_ClosedLoop/historical/*')\n",
    "sessions_check = np.load('../utils/misc_data/NewSessions_check.npy', allow_pickle = True).item()\n",
    "files          = get_Sessions(f_dir, min_nblocks, manually_remove = sessions_check['bad_days'])\n",
    "\n",
    "init_pairs    = get_SessionPairs(files, max_ndays = max_ndays)\n",
    "pairs         = init_pairs\n",
    "#pairs, scores = get_StrongTransferPairs(init_pairs, min_R2 = min_R2, train_frac = 0.5, block_constraints = sessions_check)\n",
    "n_pairs       = len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def HMMrecal_parallel(params_dict, baseOpts, train_x, cursorPos, test_x, test_y ):\n",
    "    '''Inputs are:\n",
    "    \n",
    "        params_dict (dictionary) - entries are values to sweep for associated parameters\n",
    "        baseOpts (dictionary)    - contains unchanging HMM parameters; key-value pairs are:\n",
    "        \n",
    "            'stateTrans'  : 2D float array - n x n transition matrix model\n",
    "            'targLocs'    : 2D float array - n x 2 of target location for each state\n",
    "            'pStateStart' : 1D float array - n x 1 of start probabilities\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    grid = ParameterGrid(params_dict)\n",
    "    \n",
    "    HMMs   = [HMMRecalibration(baseOpts['stateTrans'], baseOpts['targLocs'], baseOpts['pStateStart'], params['kappa'], \n",
    "                               adjustKappa = lambda dist : 1 / (1 + np.exp(-1 * (dist - params['inflection']) * params['exp']))) for params in grid]\n",
    "    \n",
    "    args   = zip(HMMs, [full_decoder] * len(HMMs), [train_x] * len(HMMs), [cursorPos] * len(HMMs), [test_x] * len(HMMs), [test_y] * len(HMMs))                   \n",
    "    scores = Parallel(n_jobs=-1)(delayed(test_HMMrecal)(*arg) for arg in args)\n",
    "    \n",
    "    # append R^2 to each parameter set: \n",
    "    scores_dict = list(grid)\n",
    "    for score, params in zip(scores, scores_dict):\n",
    "        params['score'] = score\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "def test_HMMrecal(HMM, decoder, train_x, cursorPos, test_x, test_y):\n",
    "    '''Code for training/testing HMM recalibrated decoder. Inputs are:\n",
    "    \n",
    "        HMM (HMMRecalibration object) - hmm to use \n",
    "        decoder (sklearn-like)        - decoder following sklearn conventions\n",
    "        train_x (list)                - entries are (time x n_channels) float arrays of neural activity\n",
    "        cursorPos (list)              - entries are (time x 2) float arrays of cursor positions\n",
    "        test_x (list)                 - test set version of <neural>\n",
    "        test_y (list)                 - entries are (time x 2) float arrays of cursor error signals'''\n",
    "    \n",
    "    new_decoder = HMM.recalibrate(deepcopy(decoder), train_x, cursorPos)\n",
    "    score       = new_decoder.score(np.vstack(test_x), np.vstack(test_y))\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm import *\n",
    "from hmm_utils import prep_HMMData, get_DiscreteTargetGrid, train_HMMRecalibrate\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "\n",
    "\n",
    "# general settings:\n",
    "np.random.seed(42)\n",
    "diffs           = list()\n",
    "task            = None\n",
    "train_size      = 0.67\n",
    "probWeighted    = 'probWeighted'\n",
    "gridSize         = 20  \n",
    "stayProb         = 0.999\n",
    "\n",
    "# hyperparams to sweep:  \n",
    "sigma_sweep = [None, 1, 2, 3]\n",
    "\n",
    "params_dict = dict()\n",
    "params_dict['kappa']      = [0.5, 1, 2, 4, 6, 8]\n",
    "params_dict['inflection'] = [0.1, 10, 30, 50, 70, 100, 200, 400]  \n",
    "params_dict['exp']        = [0.0001, 0.001, 0.025, 0.05, 0.1, 0.5, 1, 2, 4]\n",
    "\n",
    "#params_dict['kappa']      = [0.5, 1,]\n",
    "#params_dict['inflection'] = [0.1, 10]  \n",
    "#params_dict['exp']        = [0.0001, 0.001, 0.025]\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "nStates       = gridSize**2\n",
    "stateTrans    = np.eye(nStates)*stayProb #Define the state transition matrix, which assumes uniform transition probability of transitioning to new state\n",
    "\n",
    "for x in range(nStates):\n",
    "    idx                = np.setdiff1d(np.arange(nStates), x)\n",
    "    stateTrans[x, idx] = (1-stayProb)/(nStates-1)\n",
    "pStateStart = np.zeros((nStates,1)) + 1/nStates\n",
    "\n",
    "baseOpts = dict()\n",
    "baseOpts['stateTrans']  = stateTrans\n",
    "baseOpts['pStateStart'] = pStateStart\n",
    "\n",
    "scores = list()\n",
    "#for i, (A_file, B_file) in enumerate([pairs[j] for j in range(22, len(pairs))]):\n",
    "for i, (A_file, B_file) in enumerate(pairs):\n",
    "    dayA, dayB              = DataStruct(A_file, alignScreens = True), DataStruct(B_file, alignScreens = True)\n",
    "\n",
    "    #dayA_blocks             = [sessions_check[A_file] if A_file in sessions_check.keys() else None][0]\n",
    "    #dayB_blocks             = [sessions_check[B_file] if B_file in sessions_check.keys() else None][0] \n",
    "    #dayA_task, dayB_task, _ = getPairTasks(dayA, dayB, task = task)\n",
    "    dayA_blocks, dayB_blocks = None, None\n",
    "    dayA_task, dayB_task    = None, None\n",
    "\n",
    "    # obtain features and cursorError targets:\n",
    "    for sigma in sigma_sweep:\n",
    "    \n",
    "        Atrain_x, Atest_x, Atrain_y, Atest_y  = getTrainTest(dayA, train_size = train_size, sigma = sigma, blocks = dayA_blocks, task = dayA_task, returnFlattened = True)    \n",
    "        Atrain_x, Atest_x  = get_BlockwiseMeanSubtracted(Atrain_x, Atest_x, concatenate = True)\n",
    "        Atrain_y           = np.concatenate(Atrain_y)\n",
    "        Atest_y            = np.concatenate(Atest_y)\n",
    "\n",
    "        Btrain_x, Btest_x, Btrain_y, Btest_y, B_cursorPos, _  = getTrainTest(dayB, train_size = train_size, sigma = sigma, blocks = dayB_blocks, task = dayB_task, \n",
    "                                                                             returnFlattened = True, returnCursor = True)    \n",
    "\n",
    "        Btrain_x, Btest_x  = get_BlockwiseMeanSubtracted(Btrain_x, Btest_x, concatenate = True)\n",
    "        Btrain_y           = np.concatenate(Btrain_y)\n",
    "        Btest_y            = np.concatenate(Btest_y)\n",
    "        B_cursorPos        = np.concatenate(B_cursorPos)\n",
    "        targetPos          = Btrain_y + B_cursorPos\n",
    "\n",
    "        full_score, full_decoder = traintest_DecoderSupervised([Atrain_x], [Atrain_x], [Atrain_y], [Atrain_y], meanRecal = False)    \n",
    "        targLocs                 = get_DiscreteTargetGrid(dayB, gridSize = gridSize, task = dayB_task)\n",
    "        baseOpts['targLocs']     = targLocs\n",
    "        \n",
    "        # parallelized parameter eval for this session-pair:\n",
    "        scores_dict = HMMrecal_parallel(params_dict, baseOpts, [Btrain_x], [B_cursorPos], [Btest_x], [Btest_y])\n",
    "        \n",
    "        # add smoothing and session-specific information\n",
    "        for param in scores_dict:\n",
    "            param['idx']       = i\n",
    "            param['diffs']     = daysBetween(dayA.date, dayB.date) \n",
    "            param['smoothing'] = [sigma if sigma is not None else 0][0]\n",
    "        scores.extend(scores_dict)\n",
    "        \n",
    "    if (i + 1) % int(np.round(len(pairs) / 10)):\n",
    "        print(np.round((i + 1) * 100 / len(pairs), 1), '% complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.swarmplot(y = scores_df['score'], x = scores_df['smoothing'], orient = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "medscores = np.median(scores, axis = 0)\n",
    "args      = np.unravel_index(medscores.argmax(), medscores.shape)\n",
    "\n",
    "print('Best weighting function: logistic with inflection = ', inflection_sweep[args[0]], ' exponent = ', exp_sweep[args[1]])\n",
    "print('Best kappa: ', kappa_sweep[args[2]])\n",
    "print('Best threshold: ', thresh_sweep[args[3]])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "x = np.linspace(0, 400, 3000)\n",
    "y = coef = 1 / (1 + np.exp(-1 * (x - inflection_sweep[args[0]]) * exp_sweep[args[1]]))\n",
    "\n",
    "figSize(5, 10)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Distance to target')\n",
    "plt.ylabel('Kappa adjustment factor')\n",
    "plt.title('Weighting function')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.swarmplot(data = scores[:, args[0], args[1], args[2], args[3]], orient = 'v')\n",
    "plt.title('Session scores (best parameters)')\n",
    "plt.ylabel('R^2 (new day)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_sweep[args[0]]\n",
    "kappa_sweep[args[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nonstationarities",
   "language": "python",
   "name": "nonstationarities"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
