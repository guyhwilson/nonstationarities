{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below does a grid search for optimal HMM values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "[sys.path.append(f) for f in glob.glob('utils/*')]\n",
    "from preprocess import DataStruct\n",
    "from firingrate import raster2FR\n",
    "from plotting_utils import figSize\n",
    "from lineplots import plotsd\n",
    "from session_utils import *\n",
    "from recalibration_utils import *\n",
    "from click_utils import *\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import FactorAnalysis, PCA\n",
    "\n",
    "\n",
    "min_nblocks    = 3       # min number of blocks for a session to be include\n",
    "max_ndays      = 30      # accept all pairs of sessions regardless of time between\n",
    "min_R2         = 0.1     # subselect days with good decoder transfer performance \n",
    "\n",
    "\n",
    "f_dir          = glob.glob('D:/T5_ClosedLoop/*')\n",
    "sessions_check = np.load('misc_data/sessions_check.npy', allow_pickle = True).item()\n",
    "files          = get_Sessions(f_dir, min_nblocks)\n",
    "\n",
    "\n",
    "init_pairs    = get_SessionPairs(files, max_ndays = max_ndays, manually_remove = sessions_check['bad_days'])\n",
    "pairs, scores = get_StrongTransferPairs(init_pairs, min_R2 = min_R2, train_frac = 0.5, block_constraints = sessions_check)\n",
    "n_pairs       = len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "'''\n",
    "def my_func(inflection, exp, vmKappa, rawDecTraj, stateTrans, targLocs, B_cursorPos, pStateStart):\n",
    "    Code for parallelizing HMM sweeps. Inputs are:\n",
    "    \n",
    "        inflection, exp (floats) - parameters for adjusting kappa weighting\n",
    "        vmKappa (float)          - base kappa value\n",
    "        rawDecTraj (2D array)    - time x 2 of decoder outputs\n",
    "        stateTrans (2D array)    - square transition matrix for markov states\n",
    "        targLocs (2D array)      - k x 2 array of corresponding target positions for each state\n",
    "        B_cursorPos (2D array)   - time x 2 array of cursor positions\n",
    "        pStateStart (1D array)   - starting probabilities for each state\n",
    "    \n",
    "    def adjustKappa(dist):\n",
    "        coef = 1 / (1 + np.exp(-1 * (dist - inflection) * exp))\n",
    "        return coef \n",
    "    predTarg = hmmviterbi_vonmises(rawDecTraj, stateTrans, targLocs, B_cursorPos, pStateStart, vmKappa, adjustKappa)[0]\n",
    "    \n",
    "    return predTarg\n",
    "'''\n",
    "\n",
    "\n",
    "def HMMrecal_parallel(inflection, exp, vmKappa, probThresh, decoder, neural, stateTrans, targLocs, B_cursorPos, pStateStart):\n",
    "    '''Code for parallelizing HMM sweeps. Inputs are:\n",
    "    \n",
    "        inflection, exp (floats) - parameters for adjusting kappa weighting\n",
    "        vmKappa (float)          - base kappa value\n",
    "        probThresh (float)       - subselect high probability time points; between 0 and 1 \n",
    "        decoder (sklearn)        - sklearn LinearRegression() object \n",
    "        neural (2D array)        - time x channels of neural activity\n",
    "        stateTrans (2D array)    - square transition matrix for markov states\n",
    "        targLocs (2D array)      - k x 2 array of corresponding target positions for each state\n",
    "        B_cursorPos (2D array)   - time x 2 array of cursor positions\n",
    "        pStateStart (1D array)   - starting probabilities for each state'''\n",
    "    \n",
    "    def adjustKappa(dist):\n",
    "        coef = 1 / (1 + np.exp(-1 * (dist - inflection) * exp))\n",
    "        return coef \n",
    "    \n",
    "    new_decoder = train_HMMRecalibrate(deepcopy(full_decoder), [Btrain_x], [B_cursorPos], stateTrans, pStateStart, targLocs, vmKappa, adjustKappa, probThresh)\n",
    "    return new_decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm import *\n",
    "from hmm_utils import prep_HMMData, get_DiscreteTargetGrid, train_HMMRecalibrate\n",
    "from sklearn.metrics import r2_score\n",
    "import itertools\n",
    "\n",
    "\n",
    "# general settings:\n",
    "np.random.seed(42)\n",
    "diffs           = list()\n",
    "task            = None\n",
    "train_frac      = 0.5\n",
    "sigma           = 2\n",
    "\n",
    "# HMM settings: \n",
    "gridSize         = 20  \n",
    "stayProb         = 0.99\n",
    "kappa_sweep      = [0.5, 1, 2, 4, 6, 8]\n",
    "inflection_sweep = [0.1, 10, 30, 50, 70, 100, 200, 400]  \n",
    "exp_sweep        = [0.0001, 0.001, 0.025, 0.05, 0.1, 0.5, 1, 2, 4]\n",
    "thresh_sweep     = [0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "#kappa_sweep      = [ 1, 2]\n",
    "#inflection_sweep = [50, 70]  # inflection point sweep\n",
    "#exp_sweep        = [2, 4]\n",
    "#thresh_sweep     = [0.3]\n",
    "\n",
    "\n",
    "#--------------------------\n",
    "\n",
    "nStates       = gridSize**2\n",
    "stateTrans    = np.eye(nStates)*stayProb #Define the state transition matrix, which assumes uniform transition probability of transitioning to new state\n",
    "\n",
    "for x in range(nStates):\n",
    "    idx                = np.setdiff1d(np.arange(nStates), x)\n",
    "    stateTrans[x, idx] = (1-stayProb)/(nStates-1)\n",
    "pStateStart = np.zeros((nStates,1)) + 1/nStates\n",
    "\n",
    "\n",
    "params_grid = list(itertools.product(inflection_sweep, exp_sweep, kappa_sweep, thresh_sweep))\n",
    "grid_inds   = list(itertools.product(range(len(inflection_sweep)), range(len(exp_sweep)), range(len(kappa_sweep)), range(len(thresh_sweep))))\n",
    "\n",
    "scores   = np.zeros((n_pairs, len(inflection_sweep), len(exp_sweep), len(kappa_sweep), len(thresh_sweep) )) \n",
    "diffs    = np.zeros((n_pairs,)) # track the # of days between sessions in each pairing\n",
    "\n",
    "for i, (A_file, B_file) in enumerate(pairs):  \n",
    "    dayA, dayB              = DataStruct(A_file, alignScreens = True), DataStruct(B_file, alignScreens = True)\n",
    "    diffs[i]                = daysBetween(dayA.date, dayB.date) # record number of days between sessions\n",
    "\n",
    "    dayA_blocks             = [sessions_check[A_file] if A_file in sessions_check.keys() else None][0]\n",
    "    dayB_blocks             = [sessions_check[B_file] if B_file in sessions_check.keys() else None][0] \n",
    "    dayA_task, dayB_task, _ = getPairTasks(dayA, dayB, task = task)\n",
    "\n",
    "    # obtain features and cursorError targets:\n",
    "    Atrain_x, Atest_x, Atrain_y, Atest_y                 = getTrainTest(dayA, train_frac = train_frac, sigma = sigma, blocks = dayA_blocks, task = dayA_task, return_flattened = True)\n",
    "    Btrain_x, B_cursorPos, Btrain_y, Btest_x, _, Btest_y = prep_HMMData(dayB, train_frac = train_frac, sigma = sigma, blocks = dayB_blocks, task = task, return_flattened = True)\n",
    "    targetPos                                            = Btrain_y + B_cursorPos\n",
    "    \n",
    "    full_score, full_decoder = traintest_DecoderSupervised([Atrain_x], [Atrain_x], [Atrain_y], [Atrain_y], meanRecal = False)    \n",
    "    targLocs                 = get_DiscreteTargetGrid(dayB, gridSize = gridSize, task = dayB_task)\n",
    "\n",
    "    decoders = Parallel(n_jobs=-2)(delayed(HMMrecal_parallel)(inflection, exp, vmKappa, probThresh, full_decoder, \n",
    "                                                           Btrain_x, stateTrans, targLocs, B_cursorPos, pStateStart) \n",
    "                                                           for j, (inflection, exp, vmKappa, probThresh) in enumerate(params_grid))\n",
    "    \n",
    "    for j in range(len(decoders)):\n",
    "        score             = decoders[j].score(Btest_x - Btrain_x.mean(axis = 0), Btest_y)\n",
    "        a, b, c, d        = grid_inds[j]\n",
    "        scores[i,a,b,c,d] = score\n",
    "    \n",
    "    if (i + 1) % int(np.round(len(pairs) / 10)):\n",
    "        print(np.round((i + 1) * 100 / len(pairs), 1), '% complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "x = np.linspace(0, 400, 3000)\n",
    "y = coef = 1 / (1 + np.exp(-1 * (x - inflection_sweep[args[0]]) * exp_sweep[args[1]]))\n",
    "\n",
    "figSize(5, 10)\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Distance to target')\n",
    "plt.ylabel('Kappa adjustment factor')\n",
    "plt.title('Weighting function')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.swarmplot(scores[:, args[0], args[1], args[2], args[3]], orient = 'v')\n",
    "plt.title('Session scores (best parameters)')\n",
    "plt.ylabel('R^2 (new day)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
