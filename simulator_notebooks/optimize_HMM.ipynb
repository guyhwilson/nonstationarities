{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for determining how to adjust VonMises dispersion parameter in cursor probabilistic model based on proximity to target.\n",
    "\n",
    "We'll use a logistic function (as seen below) with parametrized midpoint and steepness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/plotting/')\n",
    "sys.path.append('../utils/recalibration/')\n",
    "sys.path.append('../utils/simulation/')\n",
    "from plotting_utils import figSize\n",
    "from hmm import HMMRecalibration\n",
    "from simulation import simulateBCIFitts\n",
    "import simulation_utils\n",
    "\n",
    "\n",
    "# for a reproducible result\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define an initial decoder and initial neural tuning properties (mean firing rates and preferred directions)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "nUnits         = 192\n",
    "SNR            = 0.5\n",
    "nTrainingSteps = 10000\n",
    "\n",
    "initialTuning          = simulation_utils.generateUnits(n_units = nUnits, SNR = SNR)\n",
    "calNeural, calVelocity = simulation_utils.simulateUnitActivity(initialTuning, noise = 0.3, nSteps = nTrainingSteps)\n",
    "lr                     = LinearRegression(fit_intercept = True).fit(calNeural, calVelocity)\n",
    "D                      = np.hstack([lr.intercept_[:, np.newaxis], lr.coef_]).T\n",
    "\n",
    "# Normalize the gain of this decoder so that it will output vectors with a magnitude of 1 when the encoded velocity has a magnitude of 1. \n",
    "D[:, 0] = D[:,0] / np.linalg.norm(D[1:, :][:, 0]) / np.linalg.norm(initialTuning[:, 1])\n",
    "D[:, 1] = D[:,1] / np.linalg.norm(D[1:, :][:, 1]) / np.linalg.norm(initialTuning[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the amount of exponential smoothing used in the decoder (alpha). Values between 0.9 and 0.96 are pretty reasonable. We then do a sweep for beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../utils/simulation/simulation.py:193: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 2d, C))\n",
      "  simAct  = getNeuralTuning(currControl, decode_params)\n",
      "../utils/simulation/simulation.py:196: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 2d, C))\n",
      "  rawDecVec = getDecodedControl(simAct, decode_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "Using gain value beta =  0.9\n"
     ]
    }
   ],
   "source": [
    "cfg = dict()\n",
    "cfg['neuralTuning'] = initialTuning\n",
    "cfg['D']            = D\n",
    "cfg['alpha']        = 0.94 # amount of exponential smoothing (0.9 to 0.96 are reasonable)\n",
    "cfg['delT']         = 0.02 # define the time step (20 ms)\n",
    "cfg['nDelaySteps']  = 10   # define the simulated user's visual feedback delay (200 ms)\n",
    "cfg['nSimSteps']    = 10000\n",
    "\n",
    "possibleGain = np.linspace(0.1,2.5,10)\n",
    "cfg['beta']  = simulation_utils.gainSweep(cfg, possibleGain, verbose = True)\n",
    "\n",
    "print('Using gain value beta = ', cfg['beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simulate nonstationarity and identify optimal HMM parameters for recalibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hmm_utils\n",
    "import copy\n",
    "\n",
    "def simulate_HMM(args, cfg, n_sessions, days_between, shrinkage):\n",
    "    \n",
    "    adjustKappa             = lambda x: 1 / (1 + np.exp(-1 * (x - args['inflection']) * args['exp']))\n",
    "    targLocs                = hmm_utils.generateTargetGrid(gridSize = args['gridSize'])\n",
    "    stateTrans, pStateStart = hmm_utils.generateTransitionMatrix(gridSize = args['gridSize'], stayProb = args['stayProb'])\n",
    "\n",
    "    hmm = HMMRecalibration(stateTrans, targLocs, pStateStart, args['kappa'], adjustKappa = adjustKappa)\n",
    "\n",
    "    session_scores = np.zeros((n_sessions + 1))\n",
    "    \n",
    "    # Day 0 performance:\n",
    "    D_HMM     = np.copy(cfg['D'])\n",
    "    tuning    = np.copy(cfg['neuralTuning'])\n",
    "    \n",
    "    norecal_cfg = copy.deepcopy(cfg)\n",
    "    hmminit_cfg = copy.deepcopy(cfg)\n",
    "    hmmrep_cfg = copy.deepcopy(cfg)\n",
    "    \n",
    "    ttt     = simulateBCIFitts(cfg)['ttt']\n",
    "    session_scores[0] = np.mean(ttt)\n",
    "    \n",
    "    for i in range(n_sessions):\n",
    "        for j in range(days_between + 1):\n",
    "            # introduce daily nonstationarities between recorded sessions\n",
    "            tuning = simulation_utils.simulateTuningShift(tuning, PD_shrinkage = shrinkage, \n",
    "                                                          mean_shift = 0, renormalize = simulation_utils.sampleSNR())  \n",
    "        \n",
    "        hmmrep_cfg['neuralTuning'] = tuning\n",
    "    \n",
    "\n",
    "        # HMM recalibration (repeated):\n",
    "        results_hmmrep          = simulateBCIFitts(hmmrep_cfg) \n",
    "        targStates, pTargState  = hmm.viterbi_search(results_hmmrep['rawDecTraj'], results_hmmrep['posTraj'])\n",
    "        maxProb                 = np.max(pTargState, axis = 0)              \n",
    "\n",
    "\n",
    "        inferredTargLoc  = targLocs[targStates.astype('int').flatten(),:]\n",
    "        inferredPosErr   = inferredTargLoc - results_hmmrep['posTraj']\n",
    "        inferredTargDist = np.linalg.norm(inferredPosErr, axis = 1)\n",
    "        neural           = results_hmmrep['neuralTraj']\n",
    "        lr           = LinearRegression(fit_intercept = True).fit(neural, inferredPosErr, maxProb**2)\n",
    "        \n",
    "        '''\n",
    "        try:\n",
    "            lr           = LinearRegression(fit_intercept = True).fit(neural, inferredPosErr, maxProb**2)\n",
    "        except: \n",
    "            print(D_HMM)\n",
    "            print(np.mean(projVec))\n",
    "            print(inferredTargDir)\n",
    "            print('faridx: ', farIdx)\n",
    "            return results_hmmrep, True\n",
    "            #return inferredTargLoc, results_hmmrep['posTraj']\n",
    "        ''' \n",
    "            \n",
    "        D_HMM        = np.hstack([lr.intercept_[:, np.newaxis], lr.coef_ ]).T\n",
    "        decVec_new   = np.hstack([np.ones((neural.shape[0], 1)), neural]).dot(D_HMM)\n",
    "\n",
    "        #inferredTargDir  = inferredPosErr / inferredTargDist[:, np.newaxis]\n",
    "        farIdx           = np.where(inferredTargDist > 0.4)[0]\n",
    "        if len(farIdx) > 0:\n",
    "            D_HMM = simulation_utils.normalizeDecoderGain(D_HMM, decVec_new, inferredPosErr, thresh = 0.4)\n",
    "        else:\n",
    "            D_HMM /= np.linalg.norm(cfg['D'][:-1, :])\n",
    "                    \n",
    "        hmmrep_cfg['D']      = D_HMM\n",
    "        ttt_RepeatedRecal    = simulateBCIFitts(hmmrep_cfg)['ttt'] #Simulate BCI performance with the HMM-recalibrated decoder\n",
    "        session_scores[i+1]  = np.mean(ttt_RepeatedRecal)\n",
    "        \n",
    "    return session_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  85 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  86 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done  87 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  88 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done  91 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=-1)]: Done  92 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  93 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  95 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done  99 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 100 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 101 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 102 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 103 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 106 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 107 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 108 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 110 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 111 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 114 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 115 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 116 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 117 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 118 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 119 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done 122 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 125 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 126 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 127 tasks      | elapsed: 17.5min\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 131 tasks      | elapsed: 18.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 133 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 134 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 135 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 139 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 140 tasks      | elapsed: 19.1min\n",
      "[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=-1)]: Done 142 tasks      | elapsed: 19.9min\n",
      "[Parallel(n_jobs=-1)]: Done 143 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=-1)]: Done 145 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 147 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 148 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed: 20.4min\n",
      "[Parallel(n_jobs=-1)]: Done 151 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 152 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 153 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 21.3min\n",
      "[Parallel(n_jobs=-1)]: Done 155 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 156 tasks      | elapsed: 21.4min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 159 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=-1)]: Done 160 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 161 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 178 out of 180 | elapsed: 24.1min remaining:   16.3s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed: 24.3min finished\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../utils/preprocessing/')\n",
    "import sweep_utils\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# general settings:\n",
    "nSimSteps  = 10000\n",
    "\n",
    "# HMM settings:\n",
    "base_opts               = dict()\n",
    "base_opts['gridSize']   = 20\n",
    "base_opts['probThresh'] = 'probWeighted'\n",
    "base_opts['stayProb']   = 0.9999\n",
    "\n",
    "\n",
    "# grid search settings:\n",
    "sweep_opts = dict()\n",
    "#sweep_opts['inflection'] = np.linspace(0, 0.5, 21)\n",
    "#sweep_opts['exp']        = np.linspace(1, 40, 11)\n",
    "#sweep_opts['kappa']      = [0.5, 1, 2, 4, 8]\n",
    "\n",
    "sweep_opts['inflection'] = np.linspace(0, 0.5, 6)\n",
    "sweep_opts['exp']        = np.linspace(1, 40, 6)\n",
    "sweep_opts['kappa']      = [0.5, 1, 2, 4, 8]\n",
    "           \n",
    "args = sweep_utils.generateArgs(sweep_opts, base_opts)\n",
    "\n",
    "           \n",
    "\n",
    "#--------------------------------------------------\n",
    "session_scores = Parallel(n_jobs= -1, verbose = 11)(delayed(simulate_HMM)(*[x, cfg, 10, 1, 0.9]) for x in args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#sweep_scores = np.load('ss_opt_simulation.npy')\n",
    "\n",
    "for i, score in enumerate(session_scores):\n",
    "    args[i]['ttt'] = score\n",
    "    \n",
    "scores_df = pd.DataFrame(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>inflection</th>\n",
       "      <th>kappa</th>\n",
       "      <th>gridSize</th>\n",
       "      <th>probThresh</th>\n",
       "      <th>stayProb</th>\n",
       "      <th>ttt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.118876404494382, 10.01, 10.01, 10.01, 10.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.0916483516483517, 1.0627659574468085, 8.285...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.1103333333333332, 3.8230769230769233, 8.479...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.0983333333333334, 1.0921978021978023, 1.632...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.0694623655913977, 1.0378125, 1.107222222222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.0727956989247311, 10.01, 10.01, 10.01, 10.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.13625, 10.01, 10.01, 3.2051612903225806, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.1023333333333334, 4.02625, 10.01, 10.01, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.1095555555555556, 7.724166666666669, 10.01,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20</td>\n",
       "      <td>probWeighted</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>[1.0629787234042551, 10.01, 10.01, 10.01, 10.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      exp  inflection  kappa  gridSize    probThresh  stayProb  \\\n",
       "0     1.0         0.0    0.5        20  probWeighted    0.9999   \n",
       "1     1.0         0.0    1.0        20  probWeighted    0.9999   \n",
       "2     1.0         0.0    2.0        20  probWeighted    0.9999   \n",
       "3     1.0         0.0    4.0        20  probWeighted    0.9999   \n",
       "4     1.0         0.0    8.0        20  probWeighted    0.9999   \n",
       "..    ...         ...    ...       ...           ...       ...   \n",
       "175  40.0         0.5    0.5        20  probWeighted    0.9999   \n",
       "176  40.0         0.5    1.0        20  probWeighted    0.9999   \n",
       "177  40.0         0.5    2.0        20  probWeighted    0.9999   \n",
       "178  40.0         0.5    4.0        20  probWeighted    0.9999   \n",
       "179  40.0         0.5    8.0        20  probWeighted    0.9999   \n",
       "\n",
       "                                                   ttt  \n",
       "0    [1.118876404494382, 10.01, 10.01, 10.01, 10.01...  \n",
       "1    [1.0916483516483517, 1.0627659574468085, 8.285...  \n",
       "2    [1.1103333333333332, 3.8230769230769233, 8.479...  \n",
       "3    [1.0983333333333334, 1.0921978021978023, 1.632...  \n",
       "4    [1.0694623655913977, 1.0378125, 1.107222222222...  \n",
       "..                                                 ...  \n",
       "175  [1.0727956989247311, 10.01, 10.01, 10.01, 10.0...  \n",
       "176  [1.13625, 10.01, 10.01, 3.2051612903225806, 10...  \n",
       "177  [1.1023333333333334, 4.02625, 10.01, 10.01, 10...  \n",
       "178  [1.1095555555555556, 7.724166666666669, 10.01,...  \n",
       "179  [1.0629787234042551, 10.01, 10.01, 10.01, 10.0...  \n",
       "\n",
       "[180 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_vals = dict([(x, np.unique(scores_df[x])) for x in sweep_opts])\n",
    "unique_lens = [len(np.unique(scores_df[x])) for x in sweep_opts]\n",
    "scores_arr  = np.zeros([*unique_lens, 11])\n",
    "\n",
    "for j, B in enumerate(unique_vals['kappa']):\n",
    "    for k, n_components in enumerate(unique_vals['inflection']):\n",
    "        for l, thresh in enumerate(unique_vals['exp']):\n",
    "            subset                 = scores_df.loc[(scores_df[\"kappa\"] == B) & (scores_df[\"inflection\"] == n_components) & \n",
    "                                                   (scores_df['exp'] == thresh)]\n",
    "            scores_arr[l, k, j, :] = subset['ttt'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal kappa:  1\n",
      "optimal inflection:  0.30000000000000004\n",
      "optimal exp:  32.2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAAGQCAYAAAD2hobeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 1440x432 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt_scores = np.median(scores_arr, axis = -1)\n",
    "\n",
    "arg_kappa, arg_inflection, arg_exp = np.unravel_index(np.argmin(plt_scores), plt_scores.shape)\n",
    "\n",
    "print('optimal kappa: ', sweep_opts['kappa'][arg_kappa])\n",
    "print('optimal inflection: ', sweep_opts['inflection'][arg_inflection])\n",
    "print('optimal exp: ', sweep_opts['exp'][arg_exp])\n",
    "\n",
    "figSize(6, 20)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(plt_scores[arg_kappa, :, :], aspect = 'auto')\n",
    "#plt.title('Fixed dimensionality = ' + str(d_sweep[arg_d]) + ' (best)')\n",
    "#plt.ylabel('Threshold')\n",
    "#plt.yticks(np.arange(len(thresh_sweep)), np.round(thresh_sweep, 3))\n",
    "#plt.xlabel('B')\n",
    "#plt.xticks(np.arange(len(B_sweep)), B_sweep)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(plt_scores[:, arg_inflection, :], aspect = 'auto')\n",
    "#plt.title('Fixed B = ' + str(B_sweep[arg_B]) + ' (best)')\n",
    "#plt.xlabel('Threshold')\n",
    "#plt.xticks(np.arange(len(thresh_sweep)), np.round(thresh_sweep, 3))\n",
    "#plt.ylabel('Dimensionality')\n",
    "#plt.yticks(np.arange(len(d_sweep)), d_sweep)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(plt_scores[:, :, arg_exp], aspect = 'auto')\n",
    "#plt.title('Fixed threshold = ' + str(np.round(thresh_sweep[arg_thresh], 3)) + ' (best)')\n",
    "#plt.xlabel('B')\n",
    "#plt.xticks(np.arange(len(B_sweep)), B_sweep)\n",
    "#plt.ylabel('Dimensionality')\n",
    "#plt.yticks(np.arange(len(d_sweep)), d_sweep)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.suptitle('Average time to target following nonstationarity + stabilization', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
