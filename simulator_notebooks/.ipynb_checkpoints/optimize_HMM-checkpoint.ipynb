{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for determining how to adjust VonMises dispersion parameter in cursor probabilistic model based on proximity to target.\n",
    "\n",
    "We'll use a logistic function (as seen below) with parametrized midpoint and steepness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/plotting/')\n",
    "sys.path.append('../utils/recalibration/')\n",
    "sys.path.append('../utils/simulation/')\n",
    "from plotting_utils import figSize\n",
    "from hmm import HMMRecalibration\n",
    "from simulation import simulateBCIFitts\n",
    "import simulation_utils\n",
    "\n",
    "\n",
    "# for a reproducible result\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define an initial decoder and initial neural tuning properties (mean firing rates and preferred directions)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "nUnits         = 192\n",
    "SNR            = 0.5\n",
    "nTrainingSteps = 10000\n",
    "\n",
    "initialTuning          = simulation_utils.generateUnits(n_units = nUnits, SNR = SNR)\n",
    "calNeural, calVelocity = simulation_utils.simulateUnitActivity(initialTuning, noise = 0.3, nSteps = nTrainingSteps)\n",
    "lr                     = LinearRegression(fit_intercept = True).fit(calNeural, calVelocity)\n",
    "D                      = np.hstack([lr.intercept_[:, np.newaxis], lr.coef_]).T\n",
    "\n",
    "# Normalize the gain of this decoder so that it will output vectors with a magnitude of 1 when the encoded velocity has a magnitude of 1. \n",
    "D[:, 0] = D[:,0] / np.linalg.norm(D[1:, :][:, 0]) / np.linalg.norm(initialTuning[:, 1])\n",
    "D[:, 1] = D[:,1] / np.linalg.norm(D[1:, :][:, 1]) / np.linalg.norm(initialTuning[:, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the amount of exponential smoothing used in the decoder (alpha). Values between 0.9 and 0.96 are pretty reasonable. We then do a sweep for beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../utils/simulation/simulation.py:193: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 2d, C))\n",
      "  simAct  = getNeuralTuning(currControl, decode_params)\n",
      "../utils/simulation/simulation.py:196: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 2d, C))\n",
      "  rawDecVec = getDecodedControl(simAct, decode_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 10\n",
      "2 / 10\n",
      "3 / 10\n",
      "4 / 10\n",
      "5 / 10\n",
      "6 / 10\n",
      "7 / 10\n",
      "8 / 10\n",
      "9 / 10\n",
      "Using gain value beta =  0.9\n"
     ]
    }
   ],
   "source": [
    "cfg = dict()\n",
    "cfg['neuralTuning'] = initialTuning\n",
    "cfg['D']            = D\n",
    "cfg['alpha']        = 0.94 # amount of exponential smoothing (0.9 to 0.96 are reasonable)\n",
    "cfg['delT']         = 0.02 # define the time step (20 ms)\n",
    "cfg['nDelaySteps']  = 10   # define the simulated user's visual feedback delay (200 ms)\n",
    "cfg['nSimSteps']    = 10000\n",
    "\n",
    "possibleGain = np.linspace(0.1,2.5,10)\n",
    "cfg['beta']  = simulation_utils.gainSweep(cfg, possibleGain, verbose = True)\n",
    "\n",
    "print('Using gain value beta = ', cfg['beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simulate nonstationarity and identify optimal HMM parameters for recalibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hmm_utils\n",
    "import copy\n",
    "\n",
    "def simulate_HMM(args, cfg, n_sessions, days_between, shrinkage):\n",
    "    \n",
    "    adjustKappa             = lambda x: 1 / (1 + np.exp(-1 * (x - args['inflection']) * args['exp']))\n",
    "    targLocs                = hmm_utils.generateTargetGrid(gridSize = args['gridSize'])\n",
    "    stateTrans, pStateStart = hmm_utils.generateTransitionMatrix(gridSize = args['gridSize'], stayProb = args['stayProb'])\n",
    "\n",
    "    hmm = HMMRecalibration(stateTrans, targLocs, pStateStart, args['kappa'], adjustKappa = adjustKappa)\n",
    "\n",
    "    session_scores = np.zeros((n_sessions + 1))\n",
    "    \n",
    "    # Day 0 performance:\n",
    "    D_HMM     = np.copy(cfg['D'])\n",
    "    tuning    = np.copy(cfg['neuralTuning'])\n",
    "    \n",
    "    norecal_cfg = copy.deepcopy(cfg)\n",
    "    hmminit_cfg = copy.deepcopy(cfg)\n",
    "    hmmrep_cfg = copy.deepcopy(cfg)\n",
    "    \n",
    "    ttt     = simulateBCIFitts(cfg)['ttt']\n",
    "    session_scores[0] = np.mean(ttt)\n",
    "    \n",
    "    for i in range(n_sessions):\n",
    "        for j in range(days_between + 1):\n",
    "            # introduce daily nonstationarities between recorded sessions\n",
    "            tuning = simulation_utils.simulateTuningShift(tuning, PD_shrinkage = shrinkage, \n",
    "                                                          mean_shift = 0, renormalize = simulation_utils.sampleSNR())  \n",
    "        \n",
    "        norecal_cfg['neuralTuning'] = tuning\n",
    "        hmminit_cfg['neuralTuning'] = tuning\n",
    "        hmmrep_cfg['neuralTuning'] = tuning\n",
    "    \n",
    "\n",
    "        # HMM recalibration (repeated):\n",
    "        results_hmmrep          = simulateBCIFitts(hmmrep_cfg) \n",
    "        targStates, pTargState  = hmm.viterbi_search(results_hmmrep['rawDecTraj'], results_hmmrep['posTraj'])\n",
    "\n",
    "        inferredTargLoc  = targLocs[targStates.astype('int').flatten(),:]\n",
    "        inferredPosErr   = inferredTargLoc - results_hmmrep['posTraj']\n",
    "        inferredTargDist = np.linalg.norm(inferredPosErr, axis = 1)\n",
    "        neural           = results_hmmrep['neuralTraj']\n",
    "\n",
    "        D_HMM            = np.linalg.lstsq(np.hstack([np.ones((neural.shape[0], 1)), neural]), inferredPosErr, rcond = -1)[0]  # update previous decoder\n",
    "        decVec_new       = np.hstack([np.ones((neural.shape[0], 1)), neural]).dot(D_HMM)\n",
    "\n",
    "        inferredTargDir  = inferredPosErr / inferredTargDist[:, np.newaxis]\n",
    "        farIdx           = np.where(inferredTargDist > 0.4)[0]\n",
    "        projVec          = np.sum(np.multiply(decVec_new[farIdx, :], inferredTargDir[farIdx, :]), axis = 1)\n",
    "        D_HMM           /= np.mean(projVec)\n",
    "        \n",
    "        \n",
    "        hmmrep_cfg['D']          = D_HMM\n",
    "        ttt_RepeatedRecal        = simulateBCIFitts(hmmrep_cfg)['ttt'] #Simulate BCI performance with the HMM-recalibrated decoder\n",
    "        session_scores[i+1]  = np.mean(ttt_RepeatedRecal)\n",
    "        \n",
    "    return session_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  2.8min\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('../utils/preprocessing/')\n",
    "import sweep_utils\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# general settings:\n",
    "nSimSteps  = 10000\n",
    "\n",
    "# HMM settings:\n",
    "base_opts               = dict()\n",
    "base_opts['gridSize']   = 20\n",
    "base_opts['probThresh'] = 'probWeighted'\n",
    "base_opts['stayProb']   = 0.9999\n",
    "\n",
    "\n",
    "# grid search settings:\n",
    "sweep_opts = dict()\n",
    "#sweep_opts['inflection'] = np.linspace(0, 0.5, 21)\n",
    "#sweep_opts['exp']        = np.linspace(1, 40, 11)\n",
    "#sweep_opts['kappa']      = [0.5, 1, 2, 4, 8]\n",
    "\n",
    "sweep_opts['inflection'] = np.linspace(0, 0.5, 6)\n",
    "sweep_opts['exp']        = np.linspace(1, 40, 6)\n",
    "sweep_opts['kappa']      = [0.5, 1, 2, 4, 8]\n",
    "           \n",
    "args = sweep_utils.generateArgs(sweep_opts, base_opts)\n",
    "\n",
    "           \n",
    "\n",
    "#--------------------------------------------------\n",
    "session_scores = Parallel(n_jobs= -1, verbose = 11)(delayed(simulate_HMM)(*[x, cfg, 10, 1, 0.9]) for x in args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1d9eaa71a2fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'session_scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sweep_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-32b9016cbabe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfigSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0margx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best inflection: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minflection_sweep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best exponent: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_sweep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sweep_scores' is not defined"
     ]
    }
   ],
   "source": [
    "figSize(10, 24)\n",
    "\n",
    "argx, argy = np.unravel_index(np.argmin(sweep_scores), sweep_scores.shape)\n",
    "print('Best inflection: ', inflection_sweep[argx])\n",
    "print('Best exponent: ', exp_sweep[argy])\n",
    "\n",
    "plt.imshow(sweep_scores.T, aspect = 'auto')\n",
    "plt.xticks(np.arange(len(inflection_sweep)), np.round(inflection_sweep, 2))\n",
    "plt.yticks(np.arange(len(exp_sweep)), np.round(exp_sweep, 2))\n",
    "plt.colorbar(label = 'Time to target (sec)')\n",
    "#plt.clim([0, 2])\n",
    "plt.xlabel('Inflection offset')\n",
    "plt.ylabel('Exponent')\n",
    "plt.title('Average time-to-target for different kappa adjustment parameters', fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'session_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-904b7c4c6be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'session_scores' is not defined"
     ]
    }
   ],
   "source": [
    "session_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
